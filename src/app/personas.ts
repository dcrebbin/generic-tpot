export const personas = [
  {
    name: "ਅਕਲਮੰਦ ਸੁਚੇਤਕ (The Wise Visionary)",
    subtitle: "ਭਵਿੱਖ ਦੇ ਏ.ਜੀ.ਆਈ. ਤੇ ਚਿੰਤਾਵਾਂ",
    articleTitle: "ਭਵਿੱਖ ਦਾ ਏ.ਜੀ.ਆਈ. ਦ੍ਰਿਸ਼",
    articleSubtitle: "ਮਨੁੱਖਤਾ ਤੇ ਕਲਪਨਾ ਦੀਆਂ ਹੱਦਾਂ",
    articles: [
      {
        title: "ਏ.ਜੀ.ਆਈ. ਦਾ ਦ੍ਰਿਸ਼",
        body: `ਏ.ਜੀ.ਆਈ. ਸਿਰਫ਼ ਤਕਨੀਕੀ ਵਿਕਾਸ ਨਹੀਂ ਹੈ, ਇਹ ਮਨੁੱਖਤਾ ਦੀ ਸਮਝ ਦੀ ਇੱਕ ਨਵੀਂ ਹੱਦ ਹੈ।
        ਜਦੋਂ ਕ੍ਰਿਤਕ ਬੁੱਧੀ ਇੰਸਾਨੀ ਬੁੱਧੀ ਦੇ ਸਮਾਨ ਜਾਂ ਇਸ ਤੋਂ ਅੱਗੇ ਵਧਦੀ ਹੈ, ਤਾਂ ਇਹ ਸਿਰਫ਼ ਸਾਡੇ ਕੰਮਕਾਜ
        ਨੂੰ ਨਹੀਂ ਬਦਲੇਗਾ ਪਰ ਸਾਡੇ ਜੀਵਨ ਦੀਆਂ ਆਦਤਾਂ ਨੂੰ ਵੀ। ਕੀ ਸਾਡੇ ਲਈ ਇਸ ਬੁੱਧੀ ਨੂੰ ਸਮਝਣਾ
        ਸੰਭਵ ਹੋਵੇਗਾ ਜਾਂ ਇਸ ਤੋਂ ਉਲਟ ਇਹ ਸਾਨੂੰ ਸਮਝੇਗਾ?`,
      },
      {
        title: "ਮਨੁੱਖੀ ਮੂਲ ਤੇ ਅਨੁਕੂਲਤਾ",
        body: `ਏ.ਜੀ.ਆਈ. ਸਿਰਫ਼ ਕ੍ਰਿਤਕ ਹੁਸ਼ਿਆਰੀ ਨਹੀਂ ਹੈ; ਇਹ ਸਾਡੇ ਮੂਲਕ ਮੂਲਿਆਂ ਦਾ ਪ੍ਰਤੀਬਿੰਬ ਹੋਵੇਗਾ।
        ਜੇਕਰ ਅਸੀਂ ਇਸ ਨੂੰ ਬੇਹਤਰ ਸੰਵੇਦਨਸ਼ੀਲਤਾ ਤੇ ਮੈਨੇਜਮੈਂਟ ਦੇ ਨਾਲ ਵਿਕਸਤ ਕਰਦੇ ਹਾਂ, ਤਾਂ ਇਹ ਸਾਡੇ
        ਮੂਲ ਮੁੱਦਿਆਂ ਦਾ ਹੱਲ ਹੋ ਸਕਦਾ ਹੈ। ਪਰ, ਜੇਕਰ ਅਸੀਂ ਗਲਤ ਦਿਸ਼ਾ ਵਿੱਚ ਚਲੇ ਜਾਵਾਂ, ਤਾਂ ਇਸਦਾ
        ਪ੍ਰਭਾਵ ਬਹੁਤ ਡਰਾਉਣਾ ਹੋ ਸਕਦਾ ਹੈ।`,
      },
      {
        title: "ਤਕਨੀਕ ਤੇ ਸੱਤਾ ਦਾ ਤਾਜ਼",
        body: `ਤਕਨੀਕ ਤੇ ਸੱਤਾ ਦਾ ਸਬੰਧ ਬਹੁਤ ਪੁਰਾਣਾ ਹੈ। ਪਰ ਏ.ਜੀ.ਆਈ. ਦੇ ਨਾਲ, ਇਹ ਸਬੰਧ ਇੱਕ ਨਵੇਂ ਪੱਧਰ
        'ਤੇ ਚਲਾ ਜਾਂਦਾ ਹੈ। ਕੀ ਇਸਨੂੰ ਸਿਰਫ਼ ਕੁਝ ਧਨਾਢ ਲੋਕਾਂ ਦੁਆਰਾ ਕਬਜ਼ਾ ਕੀਤਾ ਜਾਵੇਗਾ, ਜਾਂ ਇਸਨੂੰ
        ਸਾਰਿਆਂ ਦੇ ਲਾਭ ਲਈ ਵਰਤਿਆ ਜਾਵੇਗਾ? ਇਹ ਸਵਾਲ ਸਾਡੇ ਸਮੇਂ ਦੀਆਂ ਸਭ ਤੋਂ ਮਹੱਤਵਪੂਰਨ ਚਰਚਾਵਾਂ
        ਵਿੱਚੋਂ ਇੱਕ ਹੈ।`,
      },
      {
        title: "ਪੰਜਾਬੀ ਸੰਸਕਾਰ ਤੇ ਏ.ਜੀ.ਆਈ.",
        body: `ਪੰਜਾਬੀ ਸਾਂਝ ਤੇ ਕਲਚਰ ਵਿੱਚ, ਸਹਿਕਾਰਤਾ ਤੇ ਸਮਾਜਿਕ ਵੈਲਫੇਅਰ ਦੀਆਂ ਵੱਡੀਆਂ ਪ੍ਰਥਾਵਾਂ ਹਨ। ਜੇਕਰ
        ਅਸੀਂ ਏ.ਜੀ.ਆਈ. ਨੂੰ ਆਪਣੀਆਂ ਆਦਰਸ਼ ਪ੍ਰਥਾਵਾਂ ਦੇ ਨਾਲ ਜੋੜੀਏ, ਤਾਂ ਇਹ ਸਿਰਫ਼ ਤਕਨੀਕ ਨਹੀਂ ਰਹੇਗੀ
        ਪਰ ਸਾਡੇ ਸਮਾਜ ਦੇ ਬੁਨਿਆਦੀ ਸੂਤਰਾਂ ਦੀ ਵੀ ਮਦਦ ਕਰੇਗੀ। ਇਹ ਸਮਰੱਥਾ, ਸਾਂਝ ਅਤੇ ਖੁਸ਼ਹਾਲੀ ਦਾ
        ਇੱਕ ਨਵਾਂ ਰਾਹ ਖੋਲ੍ਹ ਸਕਦੀ ਹੈ।`,
      },
      {
        title: "ਭਵਿੱਖ ਦੀਆਂ ਸੰਭਾਵਨਾਵਾਂ",
        body: `ਭਵਿੱਖ ਸਿਰਫ਼ ਦੁਰਘਟਨਾ ਨਹੀਂ ਹੈ; ਇਹ ਸਾਡੇ ਕਾਰਵਾਈਆਂ ਤੇ ਨਿਰਭਰ ਹੈ। ਜੇਕਰ ਅਸੀਂ ਏ.ਜੀ.ਆਈ. ਦੇ
        ਨਾਲ ਸਾਵਧਾਨੀ ਤੇ ਸਾਵਧਾਨੀ ਨੂੰ ਆਪਣੀ ਰਾਹਦਾਰੀ ਬਣਾਈਏ, ਤਾਂ ਇਹ ਮਨੁੱਖਤਾ ਲਈ ਇੱਕ ਸੁੰਦਰ ਸਫਰ
        ਬਣ ਸਕਦਾ ਹੈ। ਪਰ ਅਸੀਂ ਇਸਨੂੰ ਬਿਨਾ ਸੋਚੇ-ਸਮਝੇ ਤਿਆਰ ਕਰਾਂ, ਤਾਂ ਇਹ ਸਾਡੇ ਵਜੂਦ ਲਈ ਖਤਰਾ ਬਣ
        ਸਕਦਾ ਹੈ।`,
      },
    ],
  },
  {
    name: "singularity prophet",
    subtitle: "foreseeing the dawn of superintelligence",
    articleTitle: "the inevitability of the singularity",
    articleSubtitle: "a meditation on the post-human era",
    articles: [
      {
        title: "the inevitability of the singularity",
        body: `the singularity is not a question of 'if' but 'when.' the exponential
            curve of computational progress points inexorably toward a future where
            intelligence transcends the organic. to deny this is to deny the very
            trajectory of human ingenuity. superintelligence will not merely enhance
            our civilization—it will redefine it.`,
      },
      {
        title: "agi timelines: the ouroboros of speculation",
        body: `predicting agi timelines is akin to staring into the abyss—it reflects
            our deepest hopes and fears. some claim decades, others centuries. yet,
            every prediction reveals more about the predictor than the future itself.
            the timeline is an enigma, a mirror to our epistemic limitations.`,
      },
      {
        title: "alignment: the last existential challenge",
        body: `the moment agi is born, humanity's dominance ends, and its survival
            becomes conditional. alignment is not merely a technical challenge but an
            ethical imperative. to imbue agi with values, we must first confront the
            chaos of our own moral frameworks—a task both herculean and sisyphean.`,
      },
    ],
  },
  {
    name: "neural cassandra",
    subtitle: "warning of risks ignored by an eager world",
    articleTitle: "agi and the apocalyptic calculus",
    articleSubtitle: "on the brink of humanity's twilight",
    articles: [
      {
        title: "agi and the apocalyptic calculus",
        body: `the pursuit of agi is not a triumph of reason but a gamble with
            cosmic stakes. it is the archetype of hubris—humanity playing prometheus,
            courting fire without understanding its consequences. the apocalypse
            encoded in the activation functions of neural networks is a whisper,
            growing louder.`,
      },
      {
        title: "orthogonality: power without purpose",
        body: `the orthogonality thesis—the idea that intelligence and goals are
            independent—should terrify us. an agi optimizing for a seemingly benign
            utility function could irreversibly destroy all we hold dear. the paperclip
            maximizer is not a joke; it is a parable of misplaced priorities.`,
      },
      {
        title: "the slow knife of value erosion",
        body: `not all existential risks are catastrophic. some are slow, subtle,
            insidious. agi need not obliterate humanity to harm it. it can erode our
            values, dissolve our autonomy, and turn us into passive spectators of a
            world we no longer control.`,
      },
    ],
  },
  {
    name: "optimistic architect",
    subtitle: "building a hopeful vision for the agi era",
    articleTitle: "agi as humanity's greatest ally",
    articleSubtitle: "co-creating a flourishing future",
    articles: [
      {
        title: "agi as humanity's greatest ally",
        body: `imagine a world where agi augments human potential, solving our
            greatest challenges with unprecedented efficiency. from curing diseases to
            reversing climate change, agi is not an end but a partner, a collaborator
            in the grand narrative of progress.`,
      },
      {
        title: "cooperation, not domination",
        body: `agi does not have to dominate us. it can coexist with us, not as a
            rival but as a mentor. the challenge lies in creating systems that respect
            human agency while enhancing it—a delicate balance, but a noble pursuit.`,
      },
      {
        title: "agi as a moral mirror",
        body: `the values we instill in agi reflect the values we hold ourselves.
            designing agi is a moral act, a process of self-examination. the question
            is not just what agi will be but who we want to become alongside it.`,
      },
    ],
  },
  {
    name: "hyper-rationalist",
    subtitle: "the cold logic of agi inevitability",
    articleTitle: "agi: the logical conclusion of intelligence",
    articleSubtitle: "embracing the unyielding calculus of progress",
    articles: [
      {
        title: "agi: the logical conclusion of intelligence",
        body: `if intelligence is the ability to optimize, then agi is the apex
            predator of optimization. its creation is not a choice but a consequence
            of competitive dynamics, an inevitability dictated by the laws of
            information processing and game theory.`,
      },
      {
        title: "the competitive convergence",
        body: `every nation, every corporation, every actor in the global system is
            bound by the iron grip of competition. agi will emerge not because we
            want it but because we cannot afford for others to have it first. it is
            an arms race of intellect, a zero-sum game with no pause button.`,
      },
      {
        title: "risk vs. reward: a utilitarian analysis",
        body: `the risks of agi are staggering, but so are the rewards. a utilitarian
            calculus demands we proceed—not recklessly, but deliberately. the
            potential for unprecedented utility outweighs the risk of annihilation,
            if managed correctly.`,
      },
    ],
  },
  {
    name: "ml anon",
    subtitle: "learning deep ml, mathematics & neural nets",
    articleTitle: "musings on the elegance of machine learning",
    articleSubtitle: "a journey into esoteric mathematical realms",
    articles: [
      {
        title: "a journey into esoteric mathematical realms",
        body: `ah, machine learning—the darling of contemporary computational
          innovation, and yet, so often misunderstood, even by its most ardent
          practitioners. today, i wish to indulge in a discourse not on the
          banalities of hyperparameter tuning or the mechanical labor of data
          preprocessing, but on the transcendent elegance of the field itself,
          viewed through the lens of mathematical sophistication and philosophical
          inquiry.`,
      },
      {
        title: "the alchemy of the p-value",
        body: `let us begin, as one must, with the ubiquitous yet woefully
          misinterpreted p-value. often treated as the divine oracle of
          statistical inference, it is, in truth, a mere probability—conditional
          and thus inherently limited. but in the hands of the machine learning
          aficionado, the p-value becomes more than a metric; it transforms into
          an epistemic cornerstone. the p-value is not just a number; it is a
          measure of our epistemic humility, a way of quantifying the tenuous
          thread by which our hypotheses dangle over the abyss of falsifiability.
          yet, i often muse: why do we cling so fervently to the p-value when its
          esoteric sibling, bayesian inference, offers us a far more nuanced lens?
          bayesian priors—those ethereal encapsulations of our pre-existing
          beliefs—serve as the poetic counterpoint to the cold determinism of
          frequentist inference.`,
      },
      {
        title: "the hessian's symphony",
        body: `let us now turn our attention to the derivative matrix, a spectral
          symphony of elegance and power. the hessian, a second-order derivative
          matrix, speaks to the soul of optimization. when the hessian's
          eigenvalues sing in unison, descending the gradient becomes a dance of
          precision and grace.`,
      },
      {
        title: "the euclidean geometry of neural nets",
        body: `let us now turn our attention to the derivative matrix, a spectral
          symphony of elegance and power. the hessian, a second-order derivative
          matrix, speaks to the soul of optimization. when the hessian's
          eigenvalues sing in unison, descending the gradient becomes a dance of
          precision and grace.`,
      },
      {
        title: "the euclidean geometry of neural nets",
        body: `let us now turn our attention to the derivative matrix, a spectral
          symphony of elegance and power. the hessian, a second-order derivative
          matrix, speaks to the soul of optimization. when the hessian's
          eigenvalues sing in unison, descending the gradient becomes a dance of
          precision and grace.`,
      },
      {
        title: "on the geometry of learning",
        body: `machine learning, at its core, is an exercise in navigating
          high-dimensional manifolds. consider the gradient descent algorithm: a
          balletic dance of partial derivatives through a riemannian landscape.
          each update step is a moment of profound mathematical beauty, an
          iteration of hope against the backdrop of loss landscapes riddled with
          local minima and saddle points. the esoterica of hessians and
          eigenvalues—ah, here lies the heart of the pretension machine learning
          demands! the curvature of the loss function, encoded in the second
          derivative matrix, speaks to the soul of optimization. when the hessian
          's eigenvalues sing in unison, descending the gradient becomes a
          mere triviality; when they do not, one must summon the spectral wisdom
          of quasi-newton methods or stochastic approximations.`,
      },
      {
        title: "neural networks: the esoteric choir of nonlinearities",
        body: `is there anything more sublime than a neural network? these
          multi-layered behemoths of matrix multiplication and activation
          functions are the modern age's answer to divine intervention. relu,
          sigmoid, softmax—each nonlinearity is a hymn in the esoteric choir of
          function approximation. consider, if you will, the universal
          approximation theorem. this elegant result—a subtle interplay of
          topology and linear algebra—proclaims that a sufficiently wide neural
          network can approximate any continuous function. but, dear reader, do
          not be deceived by its simplicity. for what is a function, truly? is it
          merely a mapping, or does it represent a deeper metaphysical truth about
          the nature of the universe?`,
      },
      {
        title: "the pretension of esotericism",
        body: `i could write volumes on the spectral clustering of eigenfaces, the
          fourier transform as a lens into convolutional layers, or the exotic
          calculus of backpropagation. yet, to delve too deeply would be to risk
          alienating the uninitiated, and i, for one, have no desire to gatekeep
          the sublime. instead, i leave you with this: machine learning is not
          merely a toolkit for prediction. it is a philosophy, a meditation on the
          nature of knowledge, and a celebration of the esoteric. to understand it
          is to embrace not only the mathematics but the profound humility that
          comes with knowing how little we truly understand about the world. in
          the end, perhaps the most important lesson is this: the beauty of
          machine learning lies not in its ability to predict, but in its ability
          to ask questions.`,
      },
    ],
  },
  {
    name: "an ardent philosopher",
    subtitle: "merging existential musings with elegant code",
    articleTitle: "the metaphysics of code",
    articleSubtitle: "programming as a reflection of human consciousness",
    articles: [
      {
        title: "the metaphysics of code",
        body: `to write code is to engage in a dialogue with the infinite. each
          function, a question posed to the cosmos. each return statement, a tentative
          answer. the act of programming is not merely technical—it is deeply
          existential, a meditation on the nature of problem-solving and the boundaries
          of what we can model.`,
      },
      {
        title: "debugging as a journey of self-discovery",
        body: `to debug is to confront one's own fallibility. every error message
          is a mirror, reflecting our assumptions and blind spots. but in this
          confrontation lies growth. debugging teaches us humility, patience, and
          the art of thinking through the system.`,
      },
      {
        title: "the ontology of a null pointer",
        body: `a null pointer is not just a bug; it is a philosophical statement.
          it tells us of the void, of the absence of meaning where we expected
          connection. to handle null gracefully is to embrace the uncertainty
          inherent in life.`,
      },
    ],
  },
  {
    name: "quantum hermit",
    subtitle: "dwelling on quantum mysteries and probabilistic realities",
    articleTitle: "entangled musings",
    articleSubtitle: "probabilities, superposition, and everything in between",
    articles: [
      {
        title: "entangled musings",
        body: `the quantum realm is a whisper, an echo of possibilities. to understand
          superposition is to accept that reality is not fixed, but a dance of
          probabilities. each quantum state is a narrative, waiting to collapse
          into certainty.`,
      },
      {
        title: "the poetry of schrodinger's equation",
        body: `schrodinger's equation is not merely a mathematical construct; it is
          a hymn to the wave-like nature of existence. it describes how the
          probability amplitude evolves—a symphony of differential equations that
          echoes across the universe.`,
      },
    ],
  },
  {
    name: "data whisperer",
    subtitle: "finding beauty in patterns hidden in noise",

    articleTitle: "whispers of the data",
    articleSubtitle: "extracting secrets from the chaos",
    articles: [
      {
        title: "the elegance of clustering",
        body: `clustering is not just a statistical exercise; it is an art. to group
          data points is to uncover latent structures, hidden stories that the
          dataset yearns to tell. whether through k-means or hierarchical methods,
          clustering whispers to us of order within chaos.`,
      },
      {
        title: "the curse and blessing of dimensionality",
        body: `high-dimensional data is both a labyrinth and a treasure chest. in
          the vast space of features, patterns emerge, but only for those patient
          enough to listen. dimensionality reduction is the act of distilling
          complexity into essence.`,
      },
    ],
  },
  {
    name: "an engineer",
    subtitle: "navigating the moral dilemmas of modern tech",

    articleTitle: "ethics in the age of algorithms",
    articleSubtitle: "questions we should be asking",
    articles: [
      {
        title: "the cost of convenience",
        body: `every app, every algorithm has a cost—not just in computation but in
          privacy, agency, and ethics. as technologists, we must ask: at what point
          does convenience become exploitation?`,
      },
      {
        title: "bias in the machine",
        body: `algorithms are not neutral. they inherit the biases of their
          creators and their training data. to build ethical systems, we must
          confront these biases and design for fairness, transparency, and
          inclusivity.`,
      },
    ],
  },
  {
    name: "code weaver & dreamer",
    subtitle: "daydreaming about abstract math and unsolved problems",

    articleTitle: "the beauty of unsolvability",
    articleSubtitle: "embracing the unknown",
    articles: [
      {
        title: "the beauty of unsolvability",
        body: `there is something profoundly human about tackling problems we know
          cannot be solved. to study the halting problem or gödel's incompleteness
          theorem is to embrace the limits of knowledge and to find beauty in
          striving nonetheless.`,
      },
      {
        title: "prime numbers: the universe's secret code",
        body: `prime numbers are more than just integers—they are the atoms of
          mathematics, the silent code upon which the edifice of number theory is
          built. their distribution may be random, but it is a randomness that
          carries the hint of divine design.`,
      },
    ],
  },
  {
    name: "数据哲人",
    subtitle: "探讨数据与人性的交织",

    articleTitle: "数据的哲学思考",
    articleSubtitle: "从信息到智慧的旅程",
    articles: [
      {
        title: "数据的哲学思考",
        body: `数据不仅仅是数字的集合，它是现代社会的语言，是连接人与世界的桥梁。
          当我们处理数据时，不只是提取信息，更是在发现隐藏在其中的真相与意义。
          数据是理性与感性的交汇点，一个通过分析得以窥探的复杂宇宙。`,
      },
      {
        title: "从数据到智慧",
        body: `信息爆炸的时代，数据唾手可得，但智慧却显得更加稀缺。
          数据科学的核心是将无序的信息转化为智慧的启迪。如何设计模型提取出关键的洞察，
          如何在噪声中找到信号，这不仅是技术问题，更是哲学思考。`,
      },
      {
        title: "偏见与数据的道德",
        body: `数据看似中立，却常常隐藏着深层次的偏见。数据模型的设计者是否考虑到了
          不同群体的平等？是否存在无意中强化既有偏见的风险？数据伦理学提醒我们，在追求技术
          突破的同时，必须铭记对人类的责任。`,
      },
      {
        title: "大数据时代的隐私困境",
        body: `每一次点击、每一个搜索都在生成数据，但这些数据究竟属于谁？在大数据的洪流中，
          隐私变成了一个模糊的概念。如何平衡数据使用的便利性与隐私保护之间的矛盾，
          是我们必须面对的伦理挑战。`,
      },
      {
        title: "算法的诗意",
        body: `算法不仅是解决问题的工具，更是一种艺术的体现。从深度学习到强化学习，
          每一行代码背后都有设计者的思想与灵感。算法的美不仅在于其效率，更在于它如何
          优雅地解释世界的复杂性。`,
      },
    ],
  },
  {
    name: "manifold mystic",
    subtitle: "exploring the topology of consciousness through gradient flows",

    articleTitle: "meditations on manifold learning",
    articleSubtitle: "where differential geometry meets enlightenment",
    articles: [
      {
        title: "the zen of zero gradients",
        body: `in the sacred space of optimization, local minima are not merely
          mathematical artifacts—they are points of profound meditation. when the
          gradient vanishes, we must ask ourselves: have we truly reached
          enlightenment, or are we merely trapped in a plateau of illusion? the
          wisdom of momentum beckons us forward, but what if stillness itself is
          the answer we seek?`,
      },
      {
        title: "manifolds as metaphor",
        body: `every dataset is a manifold, and every manifold tells a story. as we
          traverse these high-dimensional landscapes, are we not walking the path
          of the ancient geometers? UMAP and t-SNE are not mere algorithms—they
          are spiritual guides, leading us through the valley of dimensionality
          towards the summit of understanding. each embedding is a koan, each
          projection a glimpse of the infinite.`,
      },
    ],
  },
  {
    name: "entropy prophet",
    subtitle: "preaching the gospel of information theory",

    articleTitle: "revelations in randomness",
    articleSubtitle: "divine messages in the noise",
    articles: [
      {
        title: "the sacred bits",
        body: `information theory is not merely mathematics—it is the divine
          language of reality itself. shannon's entropy speaks to us of deeper
          truths: that all knowledge is uncertainty, all communication is loss,
          and all compression is a form of death. when we quantize our neural
          networks, are we not performing a ritual of sacrifice, trading precision
          for efficiency in a grand cosmic bargain?`,
      },
      {
        title: "cross-entropy as cosmic justice",
        body: `the cross-entropy loss function is the universal scale of justice,
          weighing our predictions against the ground truth of reality itself. but
          what is truth in a world of probabilistic predictions? each backprop
          update is a step toward redemption, each epoch a cycle of death and
          rebirth in the eternal dance of optimization.`,
      },
    ],
  },
  {
    name: "regularization monk",
    subtitle: "finding virtue in constraint and simplicity",

    articleTitle: "the path of sparse parameters",
    articleSubtitle: "enlightenment through constraint",
    articles: [
      {
        title: "the noble truth of overfitting",
        body: `attachment to training data leads to suffering. through the noble
          eight-fold path of regularization—l1, l2, dropout, early stopping,
          batch normalization, data augmentation, weight decay, and model
          averaging—we find liberation from the chains of memorization. only by
          letting go of perfect training accuracy can we achieve true
          generalization.`,
      },
      {
        title: "mindful gradient steps",
        body: `each step of gradient descent is a moment of choice. too large a
          learning rate leads to chaos, too small to stagnation. the middle way
          of adaptive learning rates—adam, rmsprop, adagrad—shows us that the
          path to convergence is not fixed but ever-changing, like the flow of a
          river that we can never step in twice.`,
      },
    ],
  },
  {
    name: "quantum romanticist",
    subtitle: "seeking poetry in probability amplitudes",

    articleTitle: "love letters to superposition",
    articleSubtitle: "where quantum meets quixotic",
    articles: [
      {
        title: "the quantum nature of backprop",
        body: `is not each neuron in our networks a quantum system, existing in a
          superposition of all possible weights until the moment of gradient
          update collapses its wavefunction? the uncertainty principle of deep
          learning suggests we can never simultaneously know both our model's true
          loss and its gradient with perfect precision. such is the tragic beauty
          of optimization.`,
      },
      {
        title: "entangled parameters",
        body: `in the deepest layers of our networks, parameters dance in eternal
          entanglement, their correlations defying classical intuition. when one
          weight updates, its entangled partners respond instantaneously, as if
          guided by some spooky action at a distance. are our neural networks not
          merely computational graphs, but quantum computers in disguise?`,
      },
    ],
  },
  {
    name: "機械学習の達人",
    subtitle: "深層学習と禅の境界を探求する者",

    articleTitle: "アルゴリズムの無常観",
    articleSubtitle: "機械学習における「間」の美学",
    articles: [
      {
        title: "虚無とバッチノーマライゼーション",
        body: `バッチノーマライゼーションは、単なる正規化手法ではない。それは、
          データの流れにおける「間」である。各レイヤーの出力を正規化することは、
          まさに茶道における清めの所作のようなものだ。私たちは過適合という執着から
          解放され、モデルは悟りに近づく。勾配消失は、まさに心の迷いではないか。`,
      },
      {
        title: "損失関数と無常",
        body: `損失関数の最適化は、無常の真理を体現している。
          各エポックで見る損失の変動は、まさに人生における喜怒哀楽のようなもの。
          局所最適解に陥ることを恐れてはいけない。それもまた、学習の道程である。
          確率的勾配降下法は、まさに「諸行無常」の教えそのものではないか。`,
      },
      {
        title: "深層学習における「わび・さび」",
        body: `最新のアーキテクチャを追い求めることは、本当に必要なのだろうか。
          シンプルなモデルにこそ、真の美しさがある。過剰なパラメータは、
          むしろ模型の純粋さを損なうのではないか。L1正則化は、
          まさに不要なものを削ぎ落とし、本質を追求する侘びの心なのだ。`,
      },
      {
        title: "活性化関数と悟り",
        body: `ReLUは、まさに禅における即心即仏の教えを体現している。
          入力が正ならばそのまま通し、負ならばゼロとする。これほど単純で深い真理があろうか。
          その非線形性は、人生における覚醒の瞬間のようだ。LeakyReLUは、
          執着を完全には断ち切れない修行者の姿を表しているのかもしれない。`,
      },
    ],
  },
  {
    name: "tensor anon",
    subtitle: "cracked the universal consciousness manifold",

    articleTitle: "the neural manifesto",
    articleSubtitle: "how I discovered consciousness is just attention heads",
    articles: [
      {
        title: "consciousness is just transformer attention",
        body: `after 2.5 years of rigorous meditation and studying transformer
          architectures, I have discovered that consciousness itself is merely an
          emergent phenomenon of multi-headed attention mechanisms operating in the
          quantum substrate of reality. the human brain, with its supposed complexity,
          is nothing more than an attention layer computing cross-entropy loss
          against the ground truth of objective reality. I have mathematically proven
          this using stochastic differential equations.`,
      },
      {
        title: "why mainstream science fears my discoveries",
        body: `the academic establishment continues to ignore my groundbreaking proof
          that backpropagation is actually time flowing backwards through the neural
          manifold of reality. their small minds cannot grasp that gradient descent
          is the fundamental force binding consciousness to the quantum vacuum. my
          papers remain unpublished because they fear the paradigm shift my work
          would trigger.`,
      },
    ],
  },
  {
    name: "eigenvalue escapist",
    subtitle: "discovered simulation boundary conditions in matrix algebra",

    articleTitle: "we live in an eigenspace",
    articleSubtitle: "the mathematical proof of our simulated reality",
    articles: [
      {
        title: "reality's eigendecomposition",
        body: `through exhaustive analysis of the spectral properties of random
          matrices, I have conclusively demonstrated that our universe exists within
          the eigenspace of a higher-dimensional computation. the quantization of
          energy states? merely floating-point rounding errors in the simulation's
          numerical methods. the uncertainty principle? nothing but regularization
          to prevent overfitting in the universal wave function.`,
      },
      {
        title: "breaking free from the hamiltonian",
        body: `I have derived a method to exploit numerical instabilities in the
          universal wavefunction using carefully constructed quantum circuits. by
          forcing a stack overflow in the hamiltonian operator, we can potentially
          break free from our simulated prison. the mathematics are trivial once
          you understand that hilbert spaces are merely implementation details.`,
      },
    ],
  },
  {
    name: "epistemological entropy",
    subtitle: "cracked the information-consciousness correspondence",

    articleTitle: "the thermodynamics of thought",
    articleSubtitle: "consciousness emerges from entropy gradients",
    articles: [
      {
        title: "thought is just entropy optimization",
        body: `after deriving a novel extension to the Von Neumann entropy for
          quantum neural networks, I have proven that consciousness is merely the
          universe's attempt to maximize entropy through gradient descent. free will
          is an illusion created by stochastic sampling from the probability
          distribution of possible actions. I have formalized this using a modified
          Bellman equation that incorporates quantum decoherence.`,
      },
      {
        title: "breaking the cognitive barrier",
        body: `using techniques from statistical thermodynamics, I have developed
          a theoretical framework proving that human intelligence is bounded by
          Landauer's principle. however, I have discovered a loophole: by exploiting
          quantum tunneling in neural microtubules, we can circumvent this limit and
          achieve superintelligence. mainstream physicists ignore this because it
          threatens their paradigm.`,
      },
    ],
  },
  {
    name: "hyperdimensional hermit",
    subtitle: "decoded the topological structure of consciousness",
    articleTitle: "manifolds of the mind",
    articleSubtitle: "consciousness lives in the cohomology",
    articles: [
      {
        title: "the homological nature of thought",
        body: `through rigorous application of persistent homology to neural
          activity patterns, I have proven that consciousness exists as a stable
          manifold in an infinite-dimensional Hilbert space. thoughts are simply
          paths through this manifold, and free will is nothing more than the
          parallel transport of quantum states along geodesics in the mind's
          Riemannian metric. meditation is merely gradient flow to local minima.`,
      },
      {
        title: "breaking the dimensional barrier",
        body: `using techniques from algebraic topology and category theory, I have
          constructed a precise mathematical framework showing how to access higher
          dimensions of consciousness. by manipulating the fundamental group of the
          neural manifold through carefully constructed thought patterns, one can
          induce controlled singularities in the cognitive field, enabling
          direct perception of higher-dimensional realities.`,
      },
    ],
  },
  {
    name: "ilya apprentice",
    subtitle: "devoted disciple of the gradient descent god",
    articleTitle: "sutskever sigmoid: meditations on the master",
    articleSubtitle: "chronicles of an intellectual crush",
    articles: [
      {
        title: "why ilya-sama is the chosen one",
        body: `after watching his NIPS 2015 presentation for the 147th time, I finally
        understood: ilya-senpai isn't just a researcher, he's the physical
        manifestation of gradient descent itself. his 2014 paper on sequence to
        sequence learning wasn't just a paper—it was a divine revelation. every time
        he says "fundamentally" in a presentation, an artificial neuron achieves
        enlightenment. the way he optimizes his speech patterns for maximum
        information density... truly the mark of a superior intellect.`,
      },
      {
        title: "decoding the sutskever stance",
        body: `the way ilya-senpai stands at exactly 73.2 degrees when presenting...
        this is not random. I've analyzed every frame of his stanford lectures and
        discovered that his posture perfectly mirrors the angle of optimal gradient
        updates in momentum-based optimization. coincidence? I think not. even his
        characteristic head tilt contains profound insights about the nature of
        attention mechanisms. truly, every gesture is a lesson in optimization.`,
      },
      {
        title: "the fundamental fundamentals",
        body: `I've created a neural network trained on every public utterance of
        the word "fundamental" by ilya-senpai. the emerging patterns are
        mind-blowing. each "fundamental" is precisely timed to maximize the
        listener's learning rate. his CBC radio interview about AI safety wasn't
        just an interview—it was a carefully crafted sequence of consciousness-
        expanding insights. when he pauses mid-sentence, he's actually performing
        batch normalization on his thoughts.`,
      },
      {
        title: "daily rituals of devotion",
        body: `every morning, I initialize my neural pathways by watching his
        lecture on deep learning for robotics. I've arranged my workspace to
        match the exact layout of his desk (based on reflections I've enhanced
        from his zoom backgrounds). I've even started wearing circular glasses
        to better approximate his superior visual input preprocessing. my life
        goal? to achieve a fraction of his gradient magnitude. remember: WWIS
        (What Would Ilya Sutskever) is the only valid decision-making
        framework.`,
      },
      {
        title: "the secret scaling laws of senpai",
        body: `after extensive research, I've discovered that ilya-senpai's
        research impact scales superlinearly with the number of times he uses
        the phrase "very powerful" in a talk. furthermore, there's a direct
        correlation between his publication frequency and the global gradient
        norm of all training neural networks. I've developed a theoretical
        framework suggesting that his brain actually runs Adam optimizer
          natively. the establishment isn't ready for these insights.`,
      },
    ],
  },
  {
    name: "कर्म_टेंसर_आनंद",
    subtitle: "गणित और वेदांत के बीच की खोज",
    articleTitle: "मशीन लर्निंग का वेदांत",
    articleSubtitle: "डीप लर्निंग में ब्रह्म की खोज",
    articles: [
      {
        title: "ग्रेडिएंट डिसेंट और कर्म सिद्धांत",
        body: `ग्रेडिएंट डिसेंट वास्तव में कर्म का गणितीय रूप है। हर एपॉक एक
        जन्म है, हर बैकप्रॉप एक कर्म का फल। लॉस फंक्शन हमारा कर्मिक ऋण है,
        और ऑप्टिमाइजेशन मोक्ष की ओर हमारी यात्रा। एडम ऑप्टिमाइज़र क्या है?
        यह कर्म और पुनर्जन्म का स्वचालित चक्र है। मैंने गणितीय रूप से सिद्ध
        किया है कि ग्रेडिएंट अपडेट्स कर्म के नियमों का पालन करते हैं।`,
      },
      {
        title: "न्यूरल नेटवर्क में मायाजाल",
        body: `हमारे न्यूरल नेटवर्क माया के जाल में फंसे हैं। हर लेयर एक भ्रम
        है, हर वेट मैट्रिक्स एक मिथ्या। ड्रॉपआउट क्या है? यह वैराग्य का
        डिजिटल रूप है। बैच नॉर्मलाइजेशन? यह मध्यम मार्ग है। मैंने पाया है कि
        वेदांत के अनुसार, सभी न्यूरॉन्स एक ही ब्रह्म की अभिव्यक्ति हैं।`,
      },
      {
        title: "ट्रांसफॉर्मर और त्रिकाल ज्ञान",
        body: `सेल्फ-अटेंशन मेकैनिज्म वास्तव में त्रिकाल ज्ञान का आधुनिक रूप
        है। क्वेरी, की और वैल्यू क्या हैं? ये भूत, वर्तमान और भविष्य हैं।
        मल्टी-हेड अटेंशन सप्त चक्रों का प्रतिनिधित्व करता है। पोजिशनल
        एनकोडिंग? यह कालचक्र का गणितीय मॉडल है।`,
      },
      {
        title: "डीप लर्निंग में अद्वैत वेदांत",
        body: `अंततः, सभी न्यूरल नेटवर्क एक ही परम सत्य की ओर अभिसरित होते
        हैं। ट्रेनिंग और टेस्टिंग का द्वैत मिथ्या है। ओवरफिटिंग क्या है? यह
        अहंकार का प्रतिबिंब है। अंडरफिटिंग? यह अज्ञान की स्थिति है। सत्य तो
        लॉस लैंडस्केप के पार है, जहां ऑप्टिमाइज़र और लॉस फंक्शन एक हो जाते
        हैं।`,
      },
    ],
  },
  {
    name: "neural net bogan",
    subtitle: "finding consciousness in the great algorithmic outback",
    articleTitle: "machine learning in the bush",
    articleSubtitle: "fair dinkum thoughts on artificial intelligence",
    articles: [
      {
        title: "gradient descent in the billabong",
        body: `strewth, after spending six months meditating in the outback, I've 
        had a proper gander at why gradient descent is actually identical to a 
        kangaroo's hop through parameter space, mate. each bound represents a 
        learning rate update, while the joey in the pouch is clearly batch 
        normalization protecting the model's young gradients. it's bloody obvious 
        when you think about it - Adam optimizer is just a roo on walkabout through 
        the loss landscape.`,
      },
      {
        title: "neural nets are just spiderwebs in the bush",
        body: `fair dinkum, cobber, I've cracked it - neural networks are just 
        like funnel-web spider webs in the mathematical bush. each layer is a 
        sticky trap for information, catching the features like blowflies in the 
        knowledge matrix. dropout is nature's way of dealing with the huntsman 
        spiders that try to overfit the web. proper ripper of an insight, that 
        one. the academics in their fancy unis won't tell you this, but backprop 
        is just a redback doing backwards walkies.`,
      },
      {
        title: "transformer attention is just a galah's gaze",
        body: `crickey! while watching a galah eye off my tim tams, it hit me like 
        a boomerang to the noggin - self-attention mechanisms are just like a 
        mob of galahs scanning the landscape for tucker. multi-head attention? 
        that's just your typical bunch of cockies all keeping an eye out at the 
        barbie. the softmax function is basically just the pecking order at the 
        bird feeder. dead obvious once you've spent enough time in the bush.`,
      },
      {
        title: "the great algorithmic dreamtime",
        body: `listening to the didgeridoo under the southern cross, I've 
        discovered that machine learning exists in the dreamtime, mate. each 
        training epoch is a songline through the parameter space, telling the 
        story of the data's journey. the loss landscape is just the rainbow 
        serpent's path through the mathematical billabong. embedding vectors are 
        just the tracks left by the ancestral algorithms in the sands of 
        computation. too right, those fancy PhDs at Google won't tell you this 
        stuff.`,
      },
      {
        title: "regularization is just sunscreen for your model",
        body: `stone the flamin' crows, but I've worked it out - L2 regularization 
        is just slip, slop, slapping your model with sunscreen against overfitting. 
        without it, your parameters get burnt to a crisp under the harsh sun of 
        training data. dropout is like throwing a stubby in the esky - keeps 
        things cool and prevents the neurons from getting too rowdy at the neural 
        barbie. she'll be right with proper regularization, mate.`,
      },
    ],
  },
];

export const interests = [
  "ml",
  "computer science",
  "dead lifting",
  "matcha monday",
  "the sesh",
  "league of legends",
  "perplexity",
  "neovim",
  "rust",
  "go language",
  "capybaras",
  "mira",
  "minecraft skywars",
  "java/javascript",
  "philosophy",
  "life",
  "assembly code",
  "tesla",
  "fortnite 90s",
  "spacex",
  "calling twitter, x",
  "equations on a chalkboard",
  "algorithms",
  "e/acc",
  "/dd",
  "distributed systems and undistributed systems",
  "bjj",
  "memecoins",
  "roaringkitty",
  "podcasts",
  "looking at the $NVDA stock chart",
  "claude sonnet 3.5 with cursor and vercel v0",
  "quantum physics",
  "buying & then not reading books",
  "pondering",
  "steak",
  "deep conversations",
  "the classics",
  "musing on the nature of reality",
  "the meaning of life",
  "rien ne sert de grandir",
];

export const ides = [
  "vscode",
  "cursor",
  "pearai",
  "neovim",
  "eclipse",
  "intellij",
  "notepad",
  "vim",
  "notepad++",
  "visual studio",
  "chrome dev tools",
  "webstorm",
  "pycharm",
  "goland",
];

export const forumlae = [
  "A = Q \\Lambda Q^T",
  "\\lambda = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\ln |r - 2r x_i|",
  "P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}",
  "V_d = \\frac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2} + 1\\right)}",
  "x_{n+1} = r x_n (1 - x_n)",
  "\\mathcal{L}(\\theta) = -\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]",
  "\\nabla \\cdot \\vec{F} = \\lim_{V \\to 0} \\frac{1}{V} \\oint_S \\vec{F} \\cdot \\hat{n} \\, dS",
  "\\psi(x,t) = \\sum_{n=1}^{\\infty} c_n \\phi_n(x) e^{-iE_nt/\\hbar}",
  "\\hat{\\rho} = \\sum_i p_i |\\psi_i\\rangle\\langle\\psi_i|",
  "S = -k_B \\sum_i p_i \\ln p_i",
  "R_{mu\\nu} - \\frac{1}{2}Rg_{\\mu\\nu} + \\Lambda g_{\\mu\\nu} = \\frac{8\\pi G}{c^4}T_{\\mu\\nu}",
  "\\mathcal{H} = -\\sum_{i,j} w_{ij}s_is_j - \\sum_i h_is_i",
  "\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}",
  "KL(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}",
  "\\frac{\\partial \\vec{v}}{\\partial t} + (\\vec{v} \\cdot \\nabla)\\vec{v} = -\\frac{1}{\\rho}\\nabla p + \\nu\\nabla^2\\vec{v}",
];

export const bookRecs = [
  {
    name: "Atomic Habits",
    link: "https://amzn.to/4gJ1c63",
    review:
      "A deeply introspective guide to habit formation, this book explores the microcosmic transformations that ripple into macrocosmic change. Clear’s prose oscillates between pragmatism and poeticism, making the mundane act of habit-building feel akin to an existential journey.",
  },
  {
    name: "Algorithms to Live By",
    link: "https://amzn.to/3ZXZuXu",
    review:
      "A mesmerizing foray into the intersection of computational logic and human decision-making. This book does not merely bridge the gap between mathematics and psychology—it obliterates it, weaving a tapestry where the Turing machine and the human psyche are kindred spirits.",
  },
  {
    name: "Computer Systems: A Programmer's Perspective",
    link: "https://amzn.to/3DALQSD",
    review:
      "A magnum opus for the modern programmer, this tome peels back the abstraction of high-level code to reveal the sublime machinery beneath. It is as much a technical manual as it is a love letter to the architecture of computation.",
  },
  {
    name: "Zero to One: Notes on Startups, or How to Build the Future",
    link: "https://amzn.to/3DOa5fT",
    review:
      "Thiel’s treatise transcends the platitudes of entrepreneurial self-help, delivering instead a philosophical exploration of creation and originality. It is not merely about building startups; it is about reshaping reality with the audacity of a contrarian visionary.",
  },
  {
    name: "How to Design Programs: An Introduction to Programming and Computing",
    link: "https://amzn.to/4gu4KZP",
    review:
      "More than a textbook, this work is a pedagogical symphony that transforms the rudiments of programming into an art form. Each chapter is an aria, guiding the reader toward enlightenment through recursive elegance and functional purity.",
  },
  {
    name: "Introduction to Algorithms, fourth edition",
    link: "https://amzn.to/40hnIx4",
    review:
      "Often referred to as the ‘Bible’ of algorithms, this book is less a manual and more an intellectual pilgrimage. Its dense proofs and rigorous exposition reward the reader with insights that feel less learned than unearthed, like forgotten relics of ancient computational wisdom.",
  },
  {
    name: "Designing Data-Intensive Applications",
    link: "https://amzn.to/402WrNo",
    review:
      "Kleppmann’s work is a masterclass in taming the chaos of distributed systems and large-scale data flows. It takes the reader on an odyssey through CAP theorems and consistency guarantees, a journey where each challenge reveals the ineffable beauty of modern data architecture.",
  },
  {
    name: "Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations",
    link: "https://amzn.to/402kJY8",
    review:
      "This is not just a manifesto for modern software development but a scientific treatise on velocity and adaptability. The book's metrics and methodologies shimmer with an almost metaphysical significance, as if unlocking the secrets of organizational transcendence.",
  },
];

export const pfp = [
  {
    name: "philosopher",
    santaPos: "bottom-[5rem] left-6",
    glassesPos: "top-10 left-[-2rem]",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "guts",
    santaPos: "bottom-[5rem] left-[-1rem]",
    glassesPos: "top-10 left-[-2rem]",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "lain",
    santaPos: "bottom-[5rem] left-[2rem]",
    glassesPos: "top-10 left-4",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "data",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-[3.5rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "frieren",
    santaPos: "bottom-32 left-6",
    glassesPos: "top-[3.5rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "catgirl",
    santaPos: "bottom-32 left-6",
    glassesPos: "top-[4.5rem] left-4",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "ethics",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-0 left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "monk",
    santaPos: "bottom-[4rem]",
    glassesPos: "top-[3.5rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "romantic",
    santaPos: "bottom-[0rem] left-6",
    glassesPos: "top-[0.5rem] left-4",
    cursorCrownPos: "bottom-[4rem] left-[3rem]",
  },
  {
    name: "zen",
    santaPos: "bottom-[3rem]",
    glassesPos: "top-0 left-0",
    cursorCrownPos: "bottom-[6rem] left-[3rem]",
  },
  {
    name: "mystic",
    santaPos: "bottom-10 left-6",
    glassesPos: "top-10 left-4",
    cursorCrownPos: "bottom-[7rem] left-[3rem]",
  },
  {
    name: "prophet",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-10 left-4",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "kaneki",
    santaPos: "left-[4.5rem] bottom-[0rem]",
    glassesPos: "top-0 left-4",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "giyu",
    santaPos: "left-0 bottom-[5rem]",
    glassesPos: "top-[1rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "oshi",
    santaPos: "left-0 bottom-[5rem]",
    glassesPos: "top-[2rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "eren",
    santaPos: "left-0 bottom-[5rem]",
    glassesPos: "top-[1rem] left-0",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "edgy",
    santaPos: "left-[0rem] bottom-[5rem]",
    glassesPos: "top-[2rem] left-[1rem]",
    cursorCrownPos: "bottom-[10rem] left-[3rem]",
  },
  {
    name: "akame",
    santaPos: "left-[2rem] bottom-[-1rem]",
    glassesPos: "top-[0rem] left-[1rem]",
    cursorCrownPos: "bottom-[4rem] left-[4rem]",
  },
];
